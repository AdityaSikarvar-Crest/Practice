{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f03269a-943e-4123-8797-46b9af90d2d6",
   "metadata": {},
   "source": [
    "## Matrix Multiplication in Python Using list comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1847f650-d818-427c-bfb7-83cf75882102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58, 64], [139, 154]]\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiplication(A, B):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of matrices A and B.\n",
    "\n",
    "    Args:\n",
    "    A: First matrix (list of lists).\n",
    "    B: Second matrix (list of lists).\n",
    "\n",
    "    Returns:\n",
    "    Result of matrix multiplication (list of lists).\n",
    "    \"\"\"\n",
    "    if len(A[0]) != len(B):\n",
    "        raise ValueError(\"Number of columns in A must equal number of rows in B\")\n",
    "\n",
    "    # Number of rows and columns in resulting matrix\n",
    "    num_rows_A = len(A)\n",
    "    num_cols_B = len(B[0])\n",
    "\n",
    "    # Perform matrix multiplication using list comprehension\n",
    "    result = [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(num_cols_B)] for i in range(num_rows_A)]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "A = [[1, 2, 3],[4, 5, 6]]\n",
    "B = [[7, 8],[9, 10],[11, 12]]\n",
    "print(matrix_multiplication(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a078d-cb3e-4ca3-ba0f-804bd654695a",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation Tool in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43de28a5-267b-450f-ad7f-bbd2be647d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    def __init__(self, num_rows, num_columns):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_columns = num_columns\n",
    "        self.data = None\n",
    "\n",
    "    def generate_numeric_data(self, min_value=0, max_value=100):\n",
    "        # Generate random numeric data\n",
    "        self.data = pd.DataFrame(np.random.randint(min_value, max_value, size=(self.num_rows, self.num_columns)), \n",
    "                                 columns=[f\"Column_{i}\" for i in range(1, self.num_columns + 1)])\n",
    "    \n",
    "    def generate_categorical_data(self, categories=None, weights=None):\n",
    "        # Generate random categorical data\n",
    "        if categories is None:\n",
    "            categories = ['Category_A', 'Category_B', 'Category_C']\n",
    "        if weights is None:\n",
    "            weights = [0.5, 0.3, 0.2]\n",
    "        self.data = pd.DataFrame(np.random.choice(categories, size=(self.num_rows, self.num_columns), p=weights), \n",
    "                                 columns=[f\"Column_{i}\" for i in range(1, self.num_columns + 1)])\n",
    "    \n",
    "    def generate_dates(self, start_date='2020-01-01', end_date='2021-12-31', format='%Y-%m-%d'):\n",
    "        # Generate date data\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date )\n",
    "        self.data = pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date, periods=self.num_rows)})\n",
    "    \n",
    "    def save_data(self, filename='synthetic_data.csv'):\n",
    "        # Save generated data to a CSV file\n",
    "        self.data.to_csv(filename, index=False)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize data generator\n",
    "    data_generator = SyntheticDataGenerator(num_rows=1000, num_columns=5)\n",
    "    # Generate numeric data\n",
    "    data_generator.generate_numeric_data()\n",
    "    # Generate categorical data\n",
    "    data_generator.generate_categorical_data()\n",
    "    # Generate dates\n",
    "    data_generator.generate_dates()\n",
    "    # Save generated data to a CSV file\n",
    "    data_generator.save_data('synthetic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d239f-4c53-4278-a4f3-b6bb8087cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import the requests module to handle HTTP requests\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup for parsing HTML\n",
    "from concurrent.futures import ThreadPoolExecutor  # Import ThreadPoolExecutor for multi-threading\n",
    "import urllib.robotparser  # Import robotparser to handle robots.txt rules\n",
    "from urllib.parse import urlparse, urljoin  # Import urlparse and urljoin for URL manipulation\n",
    "\n",
    "# Function to check if a URL is allowed to be scraped according to robots.txt\n",
    "def is_allowed(url, user_agent='*'):\n",
    "    # Parse the URL to get the base URL\n",
    "    parsed_url = urlparse(url)\n",
    "    base_url = f'{parsed_url.scheme}://{parsed_url.netloc}'\n",
    "    robots_url = urljoin(base_url, 'robots.txt')\n",
    "    \n",
    "    # Parse robots.txt\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    \n",
    "    # Check if the URL is allowed to be accessed\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "# Function to fetch and parse a webpage\n",
    "def fetch_page(url):\n",
    "    # Check if the URL is allowed to be scraped\n",
    "    if not is_allowed(url):\n",
    "        print(f'Scraping not allowed for {url}')\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            print(f'Successfully fetched {url}')\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            print(f'Failed to fetch {url} with status code {response.status_code}')\n",
    "    except Exception as e:\n",
    "        print(f'Exception occurred while fetching {url}: {e}')\n",
    "    return None\n",
    "\n",
    "# Function to extract all links from a webpage\n",
    "def extract_links(soup, base_url):\n",
    "    links = []\n",
    "    if soup:\n",
    "        # Find all anchor tags with href attribute\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            # Resolve relative URLs\n",
    "            full_url = urljoin(base_url, link['href'])\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "# Function to scrape a list of URLs using multiple threads\n",
    "def scrape_urls(urls, max_workers=5):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit fetch_page tasks to the ThreadPoolExecutor\n",
    "        futures = {executor.submit(fetch_page, url): url for url in urls}\n",
    "        results = []\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "# Main function to start the web scraper\n",
    "def main():\n",
    "    start_url = 'https://books.toscrape.com/'  # Replace with the URL you want to start scraping from\n",
    "    #start_url = 'https://google.com'  # Replace with the URL you want to start scraping from\n",
    "    soup = fetch_page(start_url)\n",
    "    if not soup:\n",
    "        return\n",
    "    \n",
    "    # Extract links from the start page\n",
    "    links = extract_links(soup, start_url)\n",
    "    # Scrape the extracted links\n",
    "    pages = scrape_urls(links)\n",
    "    \n",
    "    # Optionally, you can further process the scraped pages\n",
    "    for page in pages:\n",
    "        # Example: print the title of each page\n",
    "        if page:\n",
    "            title = page.find('title').get_text()\n",
    "            print(f'Page title: {title}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec8cd7-bfd3-45f5-a658-7966092b056a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
